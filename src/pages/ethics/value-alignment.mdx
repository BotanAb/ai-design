---
title: "Value Alignment"
---

import { Column, Row } from "gatsby-theme-carbon";
import './ethics.scss';

<Row className="opener">
<Column colMd={6} colLg={8}>

## AI should be designed to align with the norms and values of your user group in mind.
AI works alongside diverse, human interests. People make decisions based on any number of contextual factors, including their experiences, memories, upbringing, and cultural norms. These factors allow us to have a fundamental understanding of “right and wrong” in a wide range of contexts, at home, in the office, or elsewhere. This is second nature for humans, as we have a wealth of experiences to draw upon.

Today’s AI systems do not have these types of experiences to draw upon, so it is the job of designers and developers to collaborate with each other in order to ensure consideration of existing values. Care is required to ensure sensitivity to a wide range of cultural norms and values. As daunting as it may seem to take value systems into account, the common core of universal principles is that they are a cooperative phenomenon. Successful teams already understand that cooperation and collaboration leads to the best outcomes.

</Column>

<Column colMd={2} colLg={4}>

![Sampling](../../images/ethics/accountability/test.png)

</Column>
</Row>

<Row className="hr recommended">
<Column colSm={4}>

#### Recommended actions to take

</Column>
<Column colMd={4} colLg={4}>

01

Consider the culture that establishes the value systems you’re designing within. Whenever possible, bring in policymakers and academics that can help your team articulate relevant perspectives.

</Column>
<Column colMd={4} colLg={4}>

02

Work with design researchers to understand and reflect your users’ values. You can find out more about this process here.

</Column>
<Column colMd={4} colLg={4}>

03

Consider mapping out your understanding of your users’ values and aligning the AI’s actions accordingly with an Ethics Canvas. Values will be specific to certain use cases and affected communities. Alignment will allow users to better understand your AI’s actions and intents.

</Column>
</Row>



<Row className="hr quote">
<Column colMd={6} colLg={8}>

### “Nearly 50% of the surveyed developers believe that the humans creating AI should be responsible for considering the ramifications of the technology. Not the bosses. Not the middle managers. The coders.”

_Mark Wilson<br/>Fast Company7 on Stack Overflow’s<br/>Developer Survey Results 2018_

</Column>
<Column colMd={2} colLg={4}>

![Sampling](../../images/ethics/accountability/test.png)

</Column>
</Row>



<Row className="hr consider">
<Column colMd={6} colLg={8}>

#### To consider

 - Explainability is needed to build public confidence in disruptive technology, to promote safer practices, and to facilitate broader societal adoption.
 - There are situations where users may not have access to the full decision process that an AI might go through, e.g., financial investment algorithms.
 - Ensure an AI system’s level of transparency is clear. Users should stay generally informed on the AI's intent even when they can't access a breakdown of the AI's process.


</Column>
</Row>



<Row className="hr questions">
<Column colMd={6} colLg={8}>

#### Questions to ask your team

 - How do we build explainability into our experience without detracting from user experience or distracting from the task at hand?
 - Do certain processes or pieces of information need to be hidden from users for security or IP reasons? How is this explained to users?
 - Which segments of our AI decision processes can be articulated for users in an easily digestible and explainable fashion?


</Column>
</Row>



<Row className="hr example">
<Column colMd={6} colLg={8}>

#### Accountability example

 - The team utilizes design researchers to contact real guests in the hotels to understand their wants and needs through face-to-face user interviews.
 - The team considers their own responsibility when a hotel assistant’s feedback does not meet the needs or expectations of guests. They have implemented a feedback learning loop to better understand preferences and have highlighted the ability for a guest to turn off the AI at any point during their stay.

 ![Sampling](../../images/ethics/accountability/test.png)

</Column>
</Row>
